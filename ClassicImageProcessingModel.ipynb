{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3EBWMM702PtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS5_sUP_2D5E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Concatenate, Dropout\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "# ---- Load Dataset ----\n",
        "\n",
        "image_folder = \"/content/drive/MyDrive/fishpond_dataset/images\"  # Folder containing images\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv', encoding='unicode_escape')\n",
        "\n",
        "# ---- Separate Features ----\n",
        "numerical_features = [\"Temp\", \"TDS\"]\n",
        "image_column = \"images\"\n",
        "target_column = \"pH\"\n",
        "\n",
        "X_numerical = df[numerical_features].values  # Extract numerical data\n",
        "image_paths = df[image_column].values  # Extract image filenames\n",
        "y = df[target_column].values  # Extract target (pH values)\n",
        "\n",
        "# ---- Train-Test Split ----\n",
        "X_n_train, X_n_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n",
        "    X_numerical, image_paths, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---- Scale Numerical Features ----\n",
        "scaler = StandardScaler()\n",
        "X_n_train_scaled = scaler.fit_transform(X_n_train)\n",
        "X_n_test_scaled = scaler.transform(X_n_test)\n",
        "\n",
        "# ---- Load & Process Images ----\n",
        "img_size = (128, 128)  # Image size for CNN input\n",
        "def load_and_preprocess_image(image_name):\n",
        "    # Include .jpg extension in image path\n",
        "    img_path = os.path.join(image_folder, str(image_name) + \".jpg\")\n",
        "\n",
        "    # Check if file exists to prevent FileNotFoundError\n",
        "    if os.path.exists(img_path):\n",
        "        img = load_img(img_path, target_size=img_size)  # Load and resize image\n",
        "        img = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "        return img\n",
        "    else:\n",
        "        print(f\"Warning: Image file not found: {img_path}\")\n",
        "        return None  # Or handle the missing image appropriately\n",
        "\n",
        "# Filter out missing images to ensure correct data alignment\n",
        "valid_indices = [i for i, img in enumerate(X_img_train) if load_and_preprocess_image(img) is not None]\n",
        "\n",
        "X_img_train_processed = np.array([load_and_preprocess_image(X_img_train[i]) for i in valid_indices])\n",
        "X_n_train_filtered = X_n_train_scaled[valid_indices]\n",
        "y_train_filtered = y_train[valid_indices]\n",
        "\n",
        "valid_indices_test = [i for i, img in enumerate(X_img_test) if load_and_preprocess_image(img) is not None]\n",
        "\n",
        "X_img_test_processed = np.array([load_and_preprocess_image(X_img_test[i]) for i in valid_indices_test])\n",
        "X_n_test_filtered = X_n_test_scaled[valid_indices_test]\n",
        "y_test_filtered = y_test[valid_indices_test]\n",
        "# ---- Define CNN for Image Data ----\n",
        "image_input = Input(shape=(img_size[0], img_size[1], 3), name=\"Image_Input\")\n",
        "\n",
        "x = Conv2D(32, (3,3), activation='relu', padding='same')(image_input)\n",
        "x = MaxPooling2D((2,2))(x)\n",
        "x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2,2))(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "cnn_output = Dense(64, activation='relu')(x)\n",
        "\n",
        "# ---- Define DNN for Numerical Data ----\n",
        "numerical_input = Input(shape=(len(numerical_features),), name=\"Numerical_Input\")\n",
        "\n",
        "y = Dense(32, activation='relu')(numerical_input)\n",
        "y = Dense(16, activation='relu')(y)\n",
        "\n",
        "# ---- Merge CNN & DNN Outputs ----\n",
        "merged = Concatenate()([cnn_output, y])\n",
        "final_output = Dense(1, activation='linear')(merged)  # Linear activation for regression\n",
        "\n",
        "# ---- Build & Compile Model ----\n",
        "model = Model(inputs=[image_input, numerical_input], outputs=final_output)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# ---- Train Model ----\n",
        "history = model.fit(\n",
        "    [X_img_train_processed, X_n_train_filtered], y_train_filtered,\n",
        "    validation_data=([X_img_test_processed, X_n_test_filtered], y_test_filtered),\n",
        "    epochs=50, batch_size=32, verbose=1\n",
        ")\n",
        "\n",
        "# ---- Evaluate Model ----\n",
        "y_pred = model.predict([X_img_test_processed, X_n_test_filtered])\n",
        "mae = mean_absolute_error(y_test_filtered, y_pred)\n",
        "mse = mean_squared_error(y_test_filtered, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"MAE: {mae:.3f}, MSE: {mse:.3f}, RMSE: {rmse:.3f}\")\n",
        "\n",
        "# ---- Plot Training History ----\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Mean Absolute Error Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# ---- Save Model ----\n",
        "model.save(\"hybrid_ph_prediction.h5\")\n",
        "print(\"Model saved as 'hybrid_ph_prediction.h5'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Flatten, Conv2D, MaxPooling2D, Concatenate\n",
        ")\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ---- Load Dataset ----\n",
        "image_folder = \"/content/drive/MyDrive/fishpond_dataset/images\"\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv',\n",
        "                 encoding='unicode_escape')\n",
        "\n",
        "# ---- Separate Features ----\n",
        "numerical_features = [\"Temp\", \"TDS\"]\n",
        "image_column = \"images\"\n",
        "target_column = \"pH\"\n",
        "\n",
        "X_numerical = df[numerical_features].values  # Extract numerical data\n",
        "image_paths = df[image_column].values  # Extract image filenames\n",
        "y = df[target_column].values  # Extract target (pH values)\n",
        "\n",
        "# ---- Train-Test Split ----\n",
        "X_n_train, X_n_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n",
        "    X_numerical, image_paths, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---- Scale Numerical Features ----\n",
        "scaler = StandardScaler()\n",
        "X_n_train_scaled = scaler.fit_transform(X_n_train)\n",
        "X_n_test_scaled = scaler.transform(X_n_test)\n",
        "\n",
        "# ---- Load & Process Images ----\n",
        "img_size = (128, 128)  # Image size for CNN input\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(image_name):\n",
        "    img_path = os.path.join(image_folder, str(image_name) + \".jpg\")\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "        img = load_img(img_path, target_size=img_size)\n",
        "        img = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "        return img\n",
        "    else:\n",
        "        print(f\"Warning: Image file not found: {img_path}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Filter out missing images to ensure correct data alignment\n",
        "valid_indices = [i for i, img in enumerate(X_img_train) if load_and_preprocess_image(img) is not None]\n",
        "\n",
        "X_img_train_processed = np.array([load_and_preprocess_image(X_img_train[i]) for i in valid_indices])\n",
        "X_n_train_filtered = X_n_train_scaled[valid_indices]\n",
        "y_train_filtered = y_train[valid_indices]\n",
        "\n",
        "valid_indices_test = [i for i, img in enumerate(X_img_test) if load_and_preprocess_image(img) is not None]\n",
        "\n",
        "X_img_test_processed = np.array([load_and_preprocess_image(X_img_test[i]) for i in valid_indices_test])\n",
        "X_n_test_filtered = X_n_test_scaled[valid_indices_test]\n",
        "y_test_filtered = y_test[valid_indices_test]\n",
        "\n",
        "# ---- Define CNN for Image Data ----\n",
        "image_input = Input(shape=(img_size[0], img_size[1], 3), name=\"Image_Input\")\n",
        "\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(image_input)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "cnn_output = Dense(64, activation='relu')(x)\n",
        "\n",
        "# ---- Define DNN for Numerical Data ----\n",
        "numerical_input = Input(shape=(len(numerical_features),), name=\"Numerical_Input\")\n",
        "\n",
        "y = Dense(32, activation='relu')(numerical_input)\n",
        "y = Dense(16, activation='relu')(y)\n",
        "\n",
        "# ---- Merge CNN & DNN Outputs ----\n",
        "merged = Concatenate()([cnn_output, y])\n",
        "final_output = Dense(1, activation='linear')(merged)  # Linear activation for regression\n",
        "\n",
        "# ---- Build & Compile Model ----\n",
        "model = Model(inputs=[image_input, numerical_input], outputs=final_output)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
        "\n",
        "# ---- Train Model ----\n",
        "history = model.fit(\n",
        "    [X_img_train_processed, X_n_train_filtered], y_train_filtered,\n",
        "    validation_data=([X_img_test_processed, X_n_test_filtered], y_test_filtered),\n",
        "    epochs=10, batch_size=32, verbose=1\n",
        ")\n",
        "\n",
        "# ---- Evaluate Model ----\n",
        "y_pred = model.predict([X_img_test_processed, X_n_test_filtered])\n",
        "mae = mean_absolute_error(y_test_filtered, y_pred)\n",
        "mse = mean_squared_error(y_test_filtered, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"MAE: {mae:.3f}, MSE: {mse:.3f}, RMSE: {rmse:.3f}\")\n",
        "\n",
        "# ---- Plot Training History ----\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "plt.title('Mean Absolute Error Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# ---- Save Model ----\n",
        "#model.save(\"hybrid_ph_prediction.h5\")\n",
        "#print(\"Model saved as 'hybrid_ph_prediction.h5'.\")\n"
      ],
      "metadata": {
        "id": "_KWunMwr9okG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Training and Validation Loss\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "9D6NNwqREN2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Flatten, Conv2D, MaxPooling2D, Concatenate\n",
        ")\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ---- Load Dataset ----\n",
        "image_folder = \"/content/drive/MyDrive/fishpond_dataset/images\"\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv',\n",
        "                 encoding='unicode_escape')\n",
        "\n",
        "# ---- Separate Features ----\n",
        "numerical_features = [\"Temp\", \"TDS\"]\n",
        "image_column = \"images\"\n",
        "target_columns = [\"pH\", \"TDS\"]\n",
        "\n",
        "X_numerical = df[numerical_features].values  # Extract numerical data\n",
        "image_paths = df[image_column].values  # Extract image filenames\n",
        "y = df[target_columns].values  # Extract target (pH and TDS values)\n",
        "\n",
        "# ---- Train-Test Split ----\n",
        "X_n_train, X_n_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n",
        "    X_numerical, image_paths, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---- Scale Numerical Features ----\n",
        "scaler = StandardScaler()\n",
        "X_n_train_scaled = scaler.fit_transform(X_n_train)\n",
        "X_n_test_scaled = scaler.transform(X_n_test)\n",
        "\n",
        "# ---- Load & Process Images ----\n",
        "img_size = (128, 128)  # Image size for CNN input\n",
        "\n",
        "def load_and_preprocess_image(image_name):\n",
        "    img_path = os.path.join(image_folder, str(image_name) + \".jpg\")\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "        img = load_img(img_path, target_size=img_size)\n",
        "        img = img_to_array(img) / 255.0  # Normalize pixel values\n",
        "        return img\n",
        "    else:\n",
        "        print(f\"Warning: Image file not found: {img_path}\")\n",
        "        return None\n",
        "\n",
        "# Filter out missing images to ensure correct data alignment\n",
        "valid_indices = [i for i, img in enumerate(X_img_train) if load_and_preprocess_image(img) is not None]\n",
        "X_img_train_processed = np.array([load_and_preprocess_image(X_img_train[i]) for i in valid_indices])\n",
        "X_n_train_filtered = X_n_train_scaled[valid_indices]\n",
        "y_train_filtered = y_train[valid_indices]\n",
        "\n",
        "valid_indices_test = [i for i, img in enumerate(X_img_test) if load_and_preprocess_image(img) is not None]\n",
        "X_img_test_processed = np.array([load_and_preprocess_image(X_img_test[i]) for i in valid_indices_test])\n",
        "X_n_test_filtered = X_n_test_scaled[valid_indices_test]\n",
        "y_test_filtered = y_test[valid_indices_test]\n",
        "\n",
        "# ---- Define CNN for Image Data ----\n",
        "image_input = Input(shape=(img_size[0], img_size[1], 3), name=\"Image_Input\")\n",
        "\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(image_input)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "cnn_output = Dense(64, activation='relu')(x)\n",
        "\n",
        "# ---- Define DNN for Numerical Data ----\n",
        "numerical_input = Input(shape=(len(numerical_features),), name=\"Numerical_Input\")\n",
        "\n",
        "y = Dense(32, activation='relu')(numerical_input)\n",
        "y = Dense(16, activation='relu')(y)\n",
        "\n",
        "# ---- Merge CNN & DNN Outputs ----\n",
        "merged = Concatenate()([cnn_output, y])\n",
        "\n",
        "# Define two outputs: one for pH and one for TDS\n",
        "pH_output = Dense(1, activation='linear', name='pH')(merged)\n",
        "TDS_output = Dense(1, activation='linear', name='TDS')(merged)\n",
        "\n",
        "# ---- Build & Compile Model ----\n",
        "model = Model(inputs=[image_input, numerical_input], outputs=[pH_output, TDS_output])\n",
        "model.compile(optimizer='adam', loss='mse', metrics={'pH': ['mae', 'mse'], 'TDS': ['mae', 'mse']}) # Modified line\n",
        "\n",
        "# ---- Train Model ----\n",
        "history = model.fit(\n",
        "    [X_img_train_processed, X_n_train_filtered], [y_train_filtered[:, 0], y_train_filtered[:, 1]],\n",
        "    validation_data=([X_img_test_processed, X_n_test_filtered], [y_test_filtered[:, 0], y_test_filtered[:, 1]]),\n",
        "    epochs=25, batch_size=32, verbose=1\n",
        ")\n",
        "\n",
        "# ---- Evaluate Model ----\n",
        "y_pred = model.predict([X_img_test_processed, X_n_test_filtered])\n",
        "pH_pred, TDS_pred = y_pred[0], y_pred[1]\n",
        "\n",
        "# Calculate MAE for both pH and TDS\n",
        "pH_mae = mean_absolute_error(y_test_filtered[:, 0], pH_pred)\n",
        "TDS_mae = mean_absolute_error(y_test_filtered[:, 1], TDS_pred)\n",
        "\n",
        "print(f\"pH MAE: {pH_mae:.3f}, TDS MAE: {TDS_mae:.3f}\")\n",
        "\n",
        "# ---- Plot Training History ----\n",
        "# ---- Plot Training History ----\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot pH loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss (pH)', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss (pH)', color='orange')\n",
        "plt.title('pH Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot TDS loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss (TDS)', color='green')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss (TDS)', color='red')\n",
        "plt.title('TDS Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---- Plot MAE History ----\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot pH MAE\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['pH_mae'], label='Train MAE (pH)', color='blue')\n",
        "plt.plot(history.history['val_pH_mae'], label='Validation MAE (pH)', color='orange')\n",
        "plt.title('pH MAE Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "# Plot TDS MAE\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['TDS_mae'], label='Train MAE (TDS)', color='green')\n",
        "plt.plot(history.history['val_TDS_mae'], label='Validation MAE (TDS)', color='red')\n",
        "plt.title('TDS MAE Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-pbMdHwkFSa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test data\n",
        "final_mse, pH_mse, TDS_mse, pH_mae, pH_var, TDS_mae, TDS_var = model.evaluate(\n",
        "    [X_img_test_processed, X_n_test_filtered],\n",
        "    [y_test_filtered[:, 0], y_test_filtered[:, 1]],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(f\"Final pH MSE: {pH_mse:.4f}, Final pH MAE: {pH_mae:.4f}\")\n",
        "print(f\"Final TDS MSE: {TDS_mse:.4f}, Final TDS MAE: {TDS_mae:.4f}\")"
      ],
      "metadata": {
        "id": "pjKmSXOtSG2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}