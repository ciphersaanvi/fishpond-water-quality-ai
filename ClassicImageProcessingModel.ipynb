{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3EBWMM702PtZ","executionInfo":{"status":"ok","timestamp":1752131617070,"user_tz":-330,"elapsed":29088,"user":{"displayName":"Nishit","userId":"03663966283281352158"}},"outputId":"505135c1-fc20-44b8-db56-fbc44893d90d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"CS5_sUP_2D5E","executionInfo":{"status":"error","timestamp":1752132151716,"user_tz":-330,"elapsed":172,"user":{"displayName":"Nishit","userId":"03663966283281352158"}},"outputId":"175df889-e06d-4840-d0ac-c99e5f29d3ed"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3-1622970151.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mimage_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/fishpond_dataset/images\"\u001b[0m  \u001b[0;31m# Folder containing images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# ---- Separate Features ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv'"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Concatenate, Dropout\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","import PIL\n","from PIL import Image\n","\n","# ---- Load Dataset ----\n","\n","image_folder = \"/content/drive/MyDrive/fishpond_dataset/images\"  # Folder containing images\n","\n","df = pd.read_csv('/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv', encoding='unicode_escape')\n","\n","# ---- Separate Features ----\n","numerical_features = [\"Temp\", \"TDS\"]\n","image_column = \"images\"\n","target_column = \"pH\"\n","\n","X_numerical = df[numerical_features].values  # Extract numerical data\n","image_paths = df[image_column].values  # Extract image filenames\n","y = df[target_column].values  # Extract target (pH values)\n","\n","# ---- Train-Test Split ----\n","X_n_train, X_n_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n","    X_numerical, image_paths, y, test_size=0.2, random_state=42\n",")\n","\n","# ---- Scale Numerical Features ----\n","scaler = StandardScaler()\n","X_n_train_scaled = scaler.fit_transform(X_n_train)\n","X_n_test_scaled = scaler.transform(X_n_test)\n","\n","# ---- Load & Process Images ----\n","img_size = (128, 128)  # Image size for CNN input\n","def load_and_preprocess_image(image_name):\n","    # Include .jpg extension in image path\n","    img_path = os.path.join(image_folder, str(image_name) + \".jpg\")\n","\n","    # Check if file exists to prevent FileNotFoundError\n","    if os.path.exists(img_path):\n","        img = load_img(img_path, target_size=img_size)  # Load and resize image\n","        img = img_to_array(img) / 255.0  # Normalize pixel values\n","        return img\n","    else:\n","        print(f\"Warning: Image file not found: {img_path}\")\n","        return None  # Or handle the missing image appropriately\n","\n","# Filter out missing images to ensure correct data alignment\n","valid_indices = [i for i, img in enumerate(X_img_train) if load_and_preprocess_image(img) is not None]\n","\n","X_img_train_processed = np.array([load_and_preprocess_image(X_img_train[i]) for i in valid_indices])\n","X_n_train_filtered = X_n_train_scaled[valid_indices]\n","y_train_filtered = y_train[valid_indices]\n","\n","valid_indices_test = [i for i, img in enumerate(X_img_test) if load_and_preprocess_image(img) is not None]\n","\n","X_img_test_processed = np.array([load_and_preprocess_image(X_img_test[i]) for i in valid_indices_test])\n","X_n_test_filtered = X_n_test_scaled[valid_indices_test]\n","y_test_filtered = y_test[valid_indices_test]\n","# ---- Define CNN for Image Data ----\n","image_input = Input(shape=(img_size[0], img_size[1], 3), name=\"Image_Input\")\n","\n","x = Conv2D(32, (3,3), activation='relu', padding='same')(image_input)\n","x = MaxPooling2D((2,2))(x)\n","x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n","x = MaxPooling2D((2,2))(x)\n","x = Flatten()(x)\n","\n","cnn_output = Dense(64, activation='relu')(x)\n","\n","# ---- Define DNN for Numerical Data ----\n","numerical_input = Input(shape=(len(numerical_features),), name=\"Numerical_Input\")\n","\n","y = Dense(32, activation='relu')(numerical_input)\n","y = Dense(16, activation='relu')(y)\n","\n","# ---- Merge CNN & DNN Outputs ----\n","merged = Concatenate()([cnn_output, y])\n","final_output = Dense(1, activation='linear')(merged)  # Linear activation for regression\n","\n","# ---- Build & Compile Model ----\n","model = Model(inputs=[image_input, numerical_input], outputs=final_output)\n","model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n","\n","# ---- Train Model ----\n","history = model.fit(\n","    [X_img_train_processed, X_n_train_filtered], y_train_filtered,\n","    validation_data=([X_img_test_processed, X_n_test_filtered], y_test_filtered),\n","    epochs=50, batch_size=32, verbose=1\n",")\n","\n","# ---- Evaluate Model ----\n","y_pred = model.predict([X_img_test_processed, X_n_test_filtered])\n","mae = mean_absolute_error(y_test_filtered, y_pred)\n","mse = mean_squared_error(y_test_filtered, y_pred)\n","rmse = np.sqrt(mse)\n","\n","print(f\"MAE: {mae:.3f}, MSE: {mse:.3f}, RMSE: {rmse:.3f}\")\n","\n","# ---- Plot Training History ----\n","plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Loss Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1,2,2)\n","plt.plot(history.history['mae'], label='Train MAE')\n","plt.plot(history.history['val_mae'], label='Validation MAE')\n","plt.title('Mean Absolute Error Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('MAE')\n","plt.legend()\n","\n","plt.show()\n","\n","# ---- Save Model ----\n","model.save(\"hybrid_ph_prediction.h5\")\n","print(\"Model saved as 'hybrid_ph_prediction.h5'.\")"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import (\n","    Input, Dense, Flatten, Conv2D, MaxPooling2D, Concatenate\n",")\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","# ---- Load Dataset ----\n","image_folder = \"/content/drive/MyDrive/fishpond_dataset/images\"\n","\n","df = pd.read_csv('/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv',\n","                 encoding='unicode_escape')\n","\n","# ---- Separate Features ----\n","numerical_features = [\"Temp\", \"TDS\"]\n","image_column = \"images\"\n","target_column = \"pH\"\n","\n","X_numerical = df[numerical_features].values  # Extract numerical data\n","image_paths = df[image_column].values  # Extract image filenames\n","y = df[target_column].values  # Extract target (pH values)\n","\n","# ---- Train-Test Split ----\n","X_n_train, X_n_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n","    X_numerical, image_paths, y, test_size=0.2, random_state=42\n",")\n","\n","# ---- Scale Numerical Features ----\n","scaler = StandardScaler()\n","X_n_train_scaled = scaler.fit_transform(X_n_train)\n","X_n_test_scaled = scaler.transform(X_n_test)\n","\n","# ---- Load & Process Images ----\n","img_size = (128, 128)  # Image size for CNN input\n","\n","\n","def load_and_preprocess_image(image_name):\n","    img_path = os.path.join(image_folder, str(image_name) + \".jpg\")\n","\n","    if os.path.exists(img_path):\n","        img = load_img(img_path, target_size=img_size)\n","        img = img_to_array(img) / 255.0  # Normalize pixel values\n","        return img\n","    else:\n","        print(f\"Warning: Image file not found: {img_path}\")\n","        return None\n","\n","\n","# Filter out missing images to ensure correct data alignment\n","valid_indices = [i for i, img in enumerate(X_img_train) if load_and_preprocess_image(img) is not None]\n","\n","X_img_train_processed = np.array([load_and_preprocess_image(X_img_train[i]) for i in valid_indices])\n","X_n_train_filtered = X_n_train_scaled[valid_indices]\n","y_train_filtered = y_train[valid_indices]\n","\n","valid_indices_test = [i for i, img in enumerate(X_img_test) if load_and_preprocess_image(img) is not None]\n","\n","X_img_test_processed = np.array([load_and_preprocess_image(X_img_test[i]) for i in valid_indices_test])\n","X_n_test_filtered = X_n_test_scaled[valid_indices_test]\n","y_test_filtered = y_test[valid_indices_test]\n","\n","# ---- Define CNN for Image Data ----\n","image_input = Input(shape=(img_size[0], img_size[1], 3), name=\"Image_Input\")\n","\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(image_input)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Flatten()(x)\n","\n","cnn_output = Dense(64, activation='relu')(x)\n","\n","# ---- Define DNN for Numerical Data ----\n","numerical_input = Input(shape=(len(numerical_features),), name=\"Numerical_Input\")\n","\n","y = Dense(32, activation='relu')(numerical_input)\n","y = Dense(16, activation='relu')(y)\n","\n","# ---- Merge CNN & DNN Outputs ----\n","merged = Concatenate()([cnn_output, y])\n","final_output = Dense(1, activation='linear')(merged)  # Linear activation for regression\n","\n","# ---- Build & Compile Model ----\n","model = Model(inputs=[image_input, numerical_input], outputs=final_output)\n","model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n","\n","# ---- Train Model ----\n","history = model.fit(\n","    [X_img_train_processed, X_n_train_filtered], y_train_filtered,\n","    validation_data=([X_img_test_processed, X_n_test_filtered], y_test_filtered),\n","    epochs=10, batch_size=32, verbose=1\n",")\n","\n","# ---- Evaluate Model ----\n","y_pred = model.predict([X_img_test_processed, X_n_test_filtered])\n","mae = mean_absolute_error(y_test_filtered, y_pred)\n","mse = mean_squared_error(y_test_filtered, y_pred)\n","rmse = np.sqrt(mse)\n","\n","print(f\"MAE: {mae:.3f}, MSE: {mse:.3f}, RMSE: {rmse:.3f}\")\n","\n","# ---- Plot Training History ----\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Loss Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['mae'], label='Train MAE')\n","plt.plot(history.history['val_mae'], label='Validation MAE')\n","plt.title('Mean Absolute Error Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('MAE')\n","plt.legend()\n","\n","plt.show()\n","\n","# ---- Save Model ----\n","#model.save(\"hybrid_ph_prediction.h5\")\n","#print(\"Model saved as 'hybrid_ph_prediction.h5'.\")\n"],"metadata":{"id":"_KWunMwr9okG","executionInfo":{"status":"aborted","timestamp":1752131622888,"user_tz":-330,"elapsed":4,"user":{"displayName":"Nishit","userId":"03663966283281352158"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Final Training and Validation Loss\n","final_train_loss = history.history['loss'][-1]\n","final_val_loss = history.history['val_loss'][-1]\n","\n","print(f\"Final Training Loss: {final_train_loss:.4f}\")\n","print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n"],"metadata":{"id":"9D6NNwqREN2O","executionInfo":{"status":"aborted","timestamp":1752131622898,"user_tz":-330,"elapsed":35214,"user":{"displayName":"Nishit","userId":"03663966283281352158"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import (\n","    Input, Dense, Flatten, Conv2D, MaxPooling2D, Concatenate\n",")\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","# ---- Load Dataset ----\n","image_folder = \"/content/drive/MyDrive/fishpond_dataset/images\"\n","\n","df = pd.read_csv('/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv',\n","                 encoding='unicode_escape')\n","\n","# ---- Separate Features ----\n","numerical_features = [\"Temp\", \"TDS\"]\n","image_column = \"images\"\n","target_columns = [\"pH\", \"TDS\"]\n","\n","X_numerical = df[numerical_features].values  # Extract numerical data\n","image_paths = df[image_column].values  # Extract image filenames\n","y = df[target_columns].values  # Extract target (pH and TDS values)\n","\n","# ---- Train-Test Split ----\n","X_n_train, X_n_test, X_img_train, X_img_test, y_train, y_test = train_test_split(\n","    X_numerical, image_paths, y, test_size=0.2, random_state=42\n",")\n","\n","# ---- Scale Numerical Features ----\n","scaler = StandardScaler()\n","X_n_train_scaled = scaler.fit_transform(X_n_train)\n","X_n_test_scaled = scaler.transform(X_n_test)\n","\n","# ---- Load & Process Images ----\n","img_size = (128, 128)  # Image size for CNN input\n","\n","def load_and_preprocess_image(image_name):\n","    img_path = os.path.join(image_folder, str(image_name) + \".jpg\")\n","\n","    if os.path.exists(img_path):\n","        img = load_img(img_path, target_size=img_size)\n","        img = img_to_array(img) / 255.0  # Normalize pixel values\n","        return img\n","    else:\n","        print(f\"Warning: Image file not found: {img_path}\")\n","        return None\n","\n","# Filter out missing images to ensure correct data alignment\n","valid_indices = [i for i, img in enumerate(X_img_train) if load_and_preprocess_image(img) is not None]\n","X_img_train_processed = np.array([load_and_preprocess_image(X_img_train[i]) for i in valid_indices])\n","X_n_train_filtered = X_n_train_scaled[valid_indices]\n","y_train_filtered = y_train[valid_indices]\n","\n","valid_indices_test = [i for i, img in enumerate(X_img_test) if load_and_preprocess_image(img) is not None]\n","X_img_test_processed = np.array([load_and_preprocess_image(X_img_test[i]) for i in valid_indices_test])\n","X_n_test_filtered = X_n_test_scaled[valid_indices_test]\n","y_test_filtered = y_test[valid_indices_test]\n","\n","# ---- Define CNN for Image Data ----\n","image_input = Input(shape=(img_size[0], img_size[1], 3), name=\"Image_Input\")\n","\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(image_input)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","x = MaxPooling2D((2, 2))(x)\n","x = Flatten()(x)\n","\n","cnn_output = Dense(64, activation='relu')(x)\n","\n","# ---- Define DNN for Numerical Data ----\n","numerical_input = Input(shape=(len(numerical_features),), name=\"Numerical_Input\")\n","\n","y = Dense(32, activation='relu')(numerical_input)\n","y = Dense(16, activation='relu')(y)\n","\n","# ---- Merge CNN & DNN Outputs ----\n","merged = Concatenate()([cnn_output, y])\n","\n","# Define two outputs: one for pH and one for TDS\n","pH_output = Dense(1, activation='linear', name='pH')(merged)\n","TDS_output = Dense(1, activation='linear', name='TDS')(merged)\n","\n","# ---- Build & Compile Model ----\n","model = Model(inputs=[image_input, numerical_input], outputs=[pH_output, TDS_output])\n","model.compile(optimizer='adam', loss='mse', metrics={'pH': ['mae', 'mse'], 'TDS': ['mae', 'mse']}) # Modified line\n","\n","# ---- Train Model ----\n","history = model.fit(\n","    [X_img_train_processed, X_n_train_filtered], [y_train_filtered[:, 0], y_train_filtered[:, 1]],\n","    validation_data=([X_img_test_processed, X_n_test_filtered], [y_test_filtered[:, 0], y_test_filtered[:, 1]]),\n","    epochs=25, batch_size=32, verbose=1\n",")\n","\n","# ---- Evaluate Model ----\n","y_pred = model.predict([X_img_test_processed, X_n_test_filtered])\n","pH_pred, TDS_pred = y_pred[0], y_pred[1]\n","\n","# Calculate MAE for both pH and TDS\n","pH_mae = mean_absolute_error(y_test_filtered[:, 0], pH_pred)\n","TDS_mae = mean_absolute_error(y_test_filtered[:, 1], TDS_pred)\n","\n","print(f\"pH MAE: {pH_mae:.3f}, TDS MAE: {TDS_mae:.3f}\")\n","\n","# ---- Plot Training History ----\n","# ---- Plot Training History ----\n","plt.figure(figsize=(12, 6))\n","\n","# Plot pH loss\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Train Loss (pH)', color='blue')\n","plt.plot(history.history['val_loss'], label='Validation Loss (pH)', color='orange')\n","plt.title('pH Loss Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# Plot TDS loss\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'], label='Train Loss (TDS)', color='green')\n","plt.plot(history.history['val_loss'], label='Validation Loss (TDS)', color='red')\n","plt.title('TDS Loss Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# ---- Plot MAE History ----\n","plt.figure(figsize=(12, 6))\n","\n","# Plot pH MAE\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['pH_mae'], label='Train MAE (pH)', color='blue')\n","plt.plot(history.history['val_pH_mae'], label='Validation MAE (pH)', color='orange')\n","plt.title('pH MAE Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('MAE')\n","plt.legend()\n","\n","# Plot TDS MAE\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['TDS_mae'], label='Train MAE (TDS)', color='green')\n","plt.plot(history.history['val_TDS_mae'], label='Validation MAE (TDS)', color='red')\n","plt.title('TDS MAE Over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('MAE')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"-pbMdHwkFSa2","executionInfo":{"status":"aborted","timestamp":1752131622906,"user_tz":-330,"elapsed":35219,"user":{"displayName":"Nishit","userId":"03663966283281352158"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate model on test data\n","final_mse, pH_mse, TDS_mse, pH_mae, pH_var, TDS_mae, TDS_var = model.evaluate(\n","    [X_img_test_processed, X_n_test_filtered],\n","    [y_test_filtered[:, 0], y_test_filtered[:, 1]],\n","    verbose=1\n",")\n","\n","print(f\"Final pH MSE: {pH_mse:.4f}, Final pH MAE: {pH_mae:.4f}\")\n","print(f\"Final TDS MSE: {TDS_mse:.4f}, Final TDS MAE: {TDS_mae:.4f}\")"],"metadata":{"id":"pjKmSXOtSG2w","executionInfo":{"status":"aborted","timestamp":1752131622919,"user_tz":-330,"elapsed":4,"user":{"displayName":"Nishit","userId":"03663966283281352158"}}},"execution_count":null,"outputs":[]}]}