{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZGkiPMduObX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "metadata": {
        "id": "M1X3fwU_xNK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds=pd.read_csv('/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv', encoding='unicode_escape')"
      ],
      "metadata": {
        "id": "4jVsknQGxFRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.head(10)"
      ],
      "metadata": {
        "id": "uLPsgOQd0Dhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds['images']=ds['images']+str('.jpg')\n",
        "ds.head(10)"
      ],
      "metadata": {
        "id": "mYzUTr-3hEtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ds))"
      ],
      "metadata": {
        "id": "Bs6BOYubTM8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train (80%) and temp (20%)\n",
        "train_ds, test_ds = train_test_split(ds, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Train set: {len(train_ds)} samples\")\n",
        "print(f\"Test set: {len(test_ds)} samples\")"
      ],
      "metadata": {
        "id": "UZ8YJN9Qx8LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = '/content/drive/MyDrive/fishpond_dataset/images'\n",
        "\n",
        "# Get all image file paths\n",
        "image_paths = [os.path.join(base_path, f) for f in os.listdir(base_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Print the first three image paths\n",
        "print(image_paths[:3])"
      ],
      "metadata": {
        "id": "a5iNkUS4N3IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Load dataset\n",
        "csv_filenames = set(train_ds[\"images\"].astype(str))  # Convert to set for faster lookup\n",
        "image_filenames = set(f for f in os.listdir(base_path) if f.lower().endswith(('.png', '.jpg', '.jpeg')))\n",
        "\n",
        "# Find mismatches\n",
        "missing_in_folder = csv_filenames - image_filenames  # Images listed in CSV but not in folder\n",
        "missing_in_csv = image_filenames - csv_filenames  # Images in folder but missing from CSV\n",
        "\n",
        "print(f\"Images in CSV but NOT in folder: {len(missing_in_folder)} -> {list(missing_in_folder)[:5]}\")\n",
        "print(f\"Images in folder but NOT in CSV: {len(missing_in_csv)} -> {list(missing_in_csv)[:5]}\")\n"
      ],
      "metadata": {
        "id": "olXmw2UYgerI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on training labels and transform both train & test labels\n",
        "train_ds[[\"pH\", \"TDS\"]] = scaler.fit_transform(train_ds[[\"pH\", \"TDS\"]])\n",
        "test_ds[[\"pH\", \"TDS\"]] = scaler.transform(test_ds[[\"pH\", \"TDS\"]])\n",
        "\n",
        "\n",
        "def pixel_normalization(img):\n",
        "    return img / 255.0  # Normalize to [0,1]\n",
        "\n",
        "\n",
        "\n",
        "# Data Augmentation & Normalization\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# Ensure 'images' column contains only filenames (no directory path)\n",
        "train_ds[\"images\"] = train_ds[\"images\"].astype(str)  # Ensure it's a string\n",
        "test_ds[\"images\"] = test_ds[\"images\"].astype(str)\n",
        "\n",
        "# Create Generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_ds,\n",
        "    directory=base_path,  # Folder containing images\n",
        "    x_col=\"images\",  # Just the filenames\n",
        "    y_col=[\"pH\", \"TDS\"],  # Regression labels\n",
        "    target_size=(100, 100),\n",
        "    batch_size=32,\n",
        "    class_mode=\"raw\",\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_ds,\n",
        "    directory=base_path,\n",
        "    x_col=\"images\",\n",
        "    y_col=[\"pH\", \"TDS\"],\n",
        "    target_size=(100, 100),\n",
        "    batch_size=32,\n",
        "    class_mode=\"raw\",\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "pZoMvlpFZjcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained ResNet50 Model (Without Top Layers)\n",
        "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(100,100, 3))\n",
        "\n",
        "# Freeze base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Fine-Tuning (Unfreeze some layers)\n",
        "for layer in base_model.layers[-20:]:  # Unfreeze last 20 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add Custom Regression Head\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "#x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.001))(x)  # Added L2 Regularization\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.30)(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = Dense(2, activation=\"linear\")(x)  # Output 2 values (pH, TDS)\n",
        "\n",
        "# Define Model\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mse\", metrics=[\"mae\"])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "ub5ky2TAYnNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=test_generator,\n",
        "    epochs=10,  # Adjust based on performance\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    validation_steps=len(test_generator)\n",
        ")\n"
      ],
      "metadata": {
        "id": "EhD-OCeYis2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Pick a few sample images from test dataset\n",
        "sample_images = test_ds.sample(10)  # Randomly select 5 test samples\n",
        "\n",
        "for _, row in sample_images.iterrows():\n",
        "    img_path = os.path.join(base_path, row[\"images\"])  # Get full image path\n",
        "\n",
        "    # Load & preprocess image\n",
        "    img = image.load_img(img_path, target_size=(100, 100, 3))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = preprocess_input(img_array)  # Use same preprocessing as training\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_pH, predicted_TDS = predictions[0]\n",
        "\n",
        "    # If you applied MinMaxScaler earlier, inverse transform the predictions\n",
        "    actual_pH, actual_TDS = row[\"pH\"], row[\"TDS\"]\n",
        "    if \"scaler\" in globals():\n",
        "        predicted_pH, predicted_TDS = scaler.inverse_transform([[predicted_pH, predicted_TDS]])[0]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Image: {row['images']}\")\n",
        "    print(f\"  Actual pH: {actual_pH:.2f}, Predicted pH: {predicted_pH:.2f}\")\n",
        "    print(f\"  Actual TDS: {actual_TDS:.2f}, Predicted TDS: {predicted_TDS:.2f}\\n\")\n"
      ],
      "metadata": {
        "id": "HF6E6LE8pPVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Training & Validation Loss\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9oOMZh5Gp1wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_mae = model.evaluate(train_generator)\n",
        "val_loss, val_mae = model.evaluate(test_generator)\n",
        "\n",
        "print(f\"Train Loss: {train_loss:.2f}, Train MAE: {train_mae:.2f}\")\n",
        "print(f\"Validation Loss: {val_loss:.2f}, Validation MAE: {val_mae:.2f}\")\n",
        "\n",
        "#Train Loss: 49.64, Train MAE: 4.18\n",
        "#Validation Loss: 299.57, Validation MAE: 9.21 (100*100 pixel 10 epoch 20 unfreeze)No DA\n",
        "\n",
        "#Train Loss: 267.54, Train MAE: 10.28\n",
        "#Validation Loss: 571.42, Validation MAE: 14.04 (224*224 pixel 10 epoch 20 unfreeze)\n",
        "\n",
        "#Train Loss: 143.14, Train MAE: 7.61\n",
        "#Validation Loss: 369.10, Validation MAE: 11.16 (100*100 pixel 10 epoch 20 unfreeze 0.4dropout 0.01 Regularization)\n",
        "\n",
        "#Train Loss: 854.52, Train MAE: 19.55\n",
        "#Validation Loss: 1134.72, Validation MAE: 21.49 (100*100 pixel 10 epoch 10 unfreeze 0.4dropout 0.01 Regularization)\n",
        "\n",
        "\n",
        "#Train Loss: 352.28, Train MAE: 12.42\n",
        "#Validation Loss: 674.77, Validation MAE: 15.72 (100*100 pixel 10 epoch 10 unfreeze 0.35dropout 0.001 Regularization) from DA\n",
        "\n",
        "#Train Loss: 1146.32, Train MAE: 22.39\n",
        "#Validation Loss: 850.52, Validation MAE: 18.74 (100*100 20epoch 0 unfreezed 0.3 dropout 0.001 Regulaization 0.001 LR)\n",
        "\n",
        "#Train Loss: 399.78, Train MAE: 11.50\n",
        "#Validation Loss: 526.72, Validation MAE: 13.25 (100*100 20epoch 5 unfreezed 0.3 dropout 0.001 Regulaization 0.0001 LR)\n",
        "\n",
        "#Train Loss: 405.81, Train MAE: 12.52\n",
        "#Validation Loss: 339.85, Validation MAE: 11.14 (100*100 20epoch 20 unfreezed 0.3 dropout 0.001 Regulaization 0.0001 LR)"
      ],
      "metadata": {
        "id": "Hg328--UqFcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    \"\"\"Calculates MAPE given y_true and y_pred.\"\"\"\n",
        "    # Ensure both arrays have the same length before calculation\n",
        "    min_len = min(len(y_true), len(y_pred))\n",
        "    y_true = y_true[:min_len]\n",
        "    y_pred = y_pred[:min_len]\n",
        "\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Get predictions for the entire test dataset without batching\n",
        "y_true = test_ds[[\"pH\", \"TDS\"]].values\n",
        "y_pred = model.predict(test_generator, steps=len(test_generator), verbose=0)\n",
        "\n",
        "# Calculate and print MAPE for each output (pH and TDS)\n",
        "mape_pH = mean_absolute_percentage_error(y_true[:, 0], y_pred[:, 0])\n",
        "mape_TDS = mean_absolute_percentage_error(y_true[:, 1], y_pred[:, 1])\n",
        "min_samples = min(y_true.shape[0], y_pred.shape[0])\n",
        "y_true = y_true[:min_samples]\n",
        "y_pred = y_pred[:min_samples]\n",
        "\n",
        "print(f\"MAPE for pH: {mape_pH:.2f}%\")\n",
        "print(f\"MAPE for TDS: {mape_TDS:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TbwxLgRbQrrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Get the true values for the test set\n",
        "y_true = test_ds[[\"pH\", \"TDS\"]].values  # Extract pH and TDS columns as NumPy array\n",
        "\n",
        "y_pred = model.predict(test_generator, steps=len(test_generator), verbose=0)\n",
        "\n",
        "# Ensure y_true and y_pred have the same number of samples\n",
        "min_samples = min(y_true.shape[0], y_pred.shape[0])\n",
        "y_true = y_true[:min_samples]\n",
        "y_pred = y_pred[:min_samples]\n",
        "\n",
        "# Calculate R-squared for each output (pH and TDS)\n",
        "r2_pH = r2_score(y_true[:, 0], y_pred[:, 0])\n",
        "r2_TDS = r2_score(y_true[:, 1], y_pred[:, 1])\n",
        "\n",
        "print(f\"R-squared for pH: {r2_pH:.2f}\")\n",
        "print(f\"R-squared for TDS: {r2_TDS:.2f}\")\n"
      ],
      "metadata": {
        "id": "T6xYGEOnSf7m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}