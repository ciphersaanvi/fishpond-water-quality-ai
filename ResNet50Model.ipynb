{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZGkiPMduObX","executionInfo":{"status":"ok","timestamp":1752067831720,"user_tz":-330,"elapsed":41109,"user":{"displayName":"Nishit","userId":"03663966283281352158"}},"outputId":"2d5a5e1c-891e-4a5b-8ceb-fd05fe4d513c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications.resnet50 import preprocess_input\n","from tensorflow.keras.regularizers import l2"],"metadata":{"id":"M1X3fwU_xNK-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds=pd.read_csv('/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv', encoding='unicode_escape')"],"metadata":{"id":"4jVsknQGxFRv","executionInfo":{"status":"error","timestamp":1752067837530,"user_tz":-330,"elapsed":699,"user":{"displayName":"Nishit","userId":"03663966283281352158"}},"colab":{"base_uri":"https://localhost:8080/","height":304},"outputId":"d3e72ba7-8dcf-475c-d068-2669bff2491d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3-66969667.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/fishpond_dataset/pond_dataset.csv'"]}]},{"cell_type":"code","source":["ds.head(10)"],"metadata":{"id":"uLPsgOQd0Dhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds['images']=ds['images']+str('.jpg')\n","ds.head(10)"],"metadata":{"id":"mYzUTr-3hEtW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(ds))"],"metadata":{"id":"Bs6BOYubTM8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split into train (80%) and temp (20%)\n","train_ds, test_ds = train_test_split(ds, test_size=0.2, random_state=42)\n","\n","\n","# Print dataset sizes\n","print(f\"Train set: {len(train_ds)} samples\")\n","print(f\"Test set: {len(test_ds)} samples\")"],"metadata":{"id":"UZ8YJN9Qx8LN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","base_path = '/content/drive/MyDrive/fishpond_dataset/images'\n","\n","# Get all image file paths\n","image_paths = [os.path.join(base_path, f) for f in os.listdir(base_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","\n","# Print the first three image paths\n","print(image_paths[:3])"],"metadata":{"id":"a5iNkUS4N3IA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Load dataset\n","csv_filenames = set(train_ds[\"images\"].astype(str))  # Convert to set for faster lookup\n","image_filenames = set(f for f in os.listdir(base_path) if f.lower().endswith(('.png', '.jpg', '.jpeg')))\n","\n","# Find mismatches\n","missing_in_folder = csv_filenames - image_filenames  # Images listed in CSV but not in folder\n","missing_in_csv = image_filenames - csv_filenames  # Images in folder but missing from CSV\n","\n","print(f\"Images in CSV but NOT in folder: {len(missing_in_folder)} -> {list(missing_in_folder)[:5]}\")\n","print(f\"Images in folder but NOT in CSV: {len(missing_in_csv)} -> {list(missing_in_csv)[:5]}\")\n"],"metadata":{"id":"olXmw2UYgerI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","# Initialize StandardScaler\n","scaler = StandardScaler()\n","\n","# Fit on training labels and transform both train & test labels\n","train_ds[[\"pH\", \"TDS\"]] = scaler.fit_transform(train_ds[[\"pH\", \"TDS\"]])\n","test_ds[[\"pH\", \"TDS\"]] = scaler.transform(test_ds[[\"pH\", \"TDS\"]])\n","\n","\n","def pixel_normalization(img):\n","    return img / 255.0  # Normalize to [0,1]\n","\n","\n","\n","# Data Augmentation & Normalization\n","train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","# Ensure 'images' column contains only filenames (no directory path)\n","train_ds[\"images\"] = train_ds[\"images\"].astype(str)  # Ensure it's a string\n","test_ds[\"images\"] = test_ds[\"images\"].astype(str)\n","\n","# Create Generators\n","train_generator = train_datagen.flow_from_dataframe(\n","    dataframe=train_ds,\n","    directory=base_path,  # Folder containing images\n","    x_col=\"images\",  # Just the filenames\n","    y_col=[\"pH\", \"TDS\"],  # Regression labels\n","    target_size=(100, 100),\n","    batch_size=32,\n","    class_mode=\"raw\",\n","    shuffle=True\n",")\n","\n","test_generator = test_datagen.flow_from_dataframe(\n","    dataframe=test_ds,\n","    directory=base_path,\n","    x_col=\"images\",\n","    y_col=[\"pH\", \"TDS\"],\n","    target_size=(100, 100),\n","    batch_size=32,\n","    class_mode=\"raw\",\n","    shuffle=False\n",")\n"],"metadata":{"id":"pZoMvlpFZjcb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Pretrained ResNet50 Model (Without Top Layers)\n","base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(100,100, 3))\n","\n","# Freeze base model layers\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Fine-Tuning (Unfreeze some layers)\n","for layer in base_model.layers[-20:]:  # Unfreeze last 20 layers\n","    layer.trainable = True\n","\n","# Add Custom Regression Head\n","x = GlobalAveragePooling2D()(base_model.output)\n","#x = Dense(512, activation=\"relu\", kernel_regularizer=l2(0.001))(x)  # Added L2 Regularization\n","x = Dense(512, activation=\"relu\")(x)\n","x = Dropout(0.30)(x)\n","x = Dense(256, activation=\"relu\")(x)\n","x = Dense(2, activation=\"linear\")(x)  # Output 2 values (pH, TDS)\n","\n","# Define Model\n","model = Model(inputs=base_model.input, outputs=x)\n","\n","# Compile Model\n","model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mse\", metrics=[\"mae\"])\n","\n","# Model Summary\n","model.summary()\n"],"metadata":{"id":"ub5ky2TAYnNT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Train Model\n","history = model.fit(\n","    train_generator,\n","    validation_data=test_generator,\n","    epochs=10,  # Adjust based on performance\n","    steps_per_epoch=len(train_generator),\n","    validation_steps=len(test_generator)\n",")\n"],"metadata":{"id":"EhD-OCeYis2_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","from tensorflow.keras.preprocessing import image\n","\n","# Pick a few sample images from test dataset\n","sample_images = test_ds.sample(10)  # Randomly select 5 test samples\n","\n","for _, row in sample_images.iterrows():\n","    img_path = os.path.join(base_path, row[\"images\"])  # Get full image path\n","\n","    # Load & preprocess image\n","    img = image.load_img(img_path, target_size=(100, 100, 3))\n","    img_array = image.img_to_array(img)\n","    img_array = preprocess_input(img_array)  # Use same preprocessing as training\n","    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n","\n","    # Make prediction\n","    predictions = model.predict(img_array)\n","    predicted_pH, predicted_TDS = predictions[0]\n","\n","    # If you applied MinMaxScaler earlier, inverse transform the predictions\n","    actual_pH, actual_TDS = row[\"pH\"], row[\"TDS\"]\n","    if \"scaler\" in globals():\n","        predicted_pH, predicted_TDS = scaler.inverse_transform([[predicted_pH, predicted_TDS]])[0]\n","\n","    # Print results\n","    print(f\"Image: {row['images']}\")\n","    print(f\"  Actual pH: {actual_pH:.2f}, Predicted pH: {predicted_pH:.2f}\")\n","    print(f\"  Actual TDS: {actual_TDS:.2f}, Predicted TDS: {predicted_TDS:.2f}\\n\")\n"],"metadata":{"id":"HF6E6LE8pPVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Plot Training & Validation Loss\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"9oOMZh5Gp1wF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss, train_mae = model.evaluate(train_generator)\n","val_loss, val_mae = model.evaluate(test_generator)\n","\n","print(f\"Train Loss: {train_loss:.2f}, Train MAE: {train_mae:.2f}\")\n","print(f\"Validation Loss: {val_loss:.2f}, Validation MAE: {val_mae:.2f}\")\n","\n","#Train Loss: 49.64, Train MAE: 4.18\n","#Validation Loss: 299.57, Validation MAE: 9.21 (100*100 pixel 10 epoch 20 unfreeze)No DA\n","\n","#Train Loss: 267.54, Train MAE: 10.28\n","#Validation Loss: 571.42, Validation MAE: 14.04 (224*224 pixel 10 epoch 20 unfreeze)\n","\n","#Train Loss: 143.14, Train MAE: 7.61\n","#Validation Loss: 369.10, Validation MAE: 11.16 (100*100 pixel 10 epoch 20 unfreeze 0.4dropout 0.01 Regularization)\n","\n","#Train Loss: 854.52, Train MAE: 19.55\n","#Validation Loss: 1134.72, Validation MAE: 21.49 (100*100 pixel 10 epoch 10 unfreeze 0.4dropout 0.01 Regularization)\n","\n","\n","#Train Loss: 352.28, Train MAE: 12.42\n","#Validation Loss: 674.77, Validation MAE: 15.72 (100*100 pixel 10 epoch 10 unfreeze 0.35dropout 0.001 Regularization) from DA\n","\n","#Train Loss: 1146.32, Train MAE: 22.39\n","#Validation Loss: 850.52, Validation MAE: 18.74 (100*100 20epoch 0 unfreezed 0.3 dropout 0.001 Regulaization 0.001 LR)\n","\n","#Train Loss: 399.78, Train MAE: 11.50\n","#Validation Loss: 526.72, Validation MAE: 13.25 (100*100 20epoch 5 unfreezed 0.3 dropout 0.001 Regulaization 0.0001 LR)\n","\n","#Train Loss: 405.81, Train MAE: 12.52\n","#Validation Loss: 339.85, Validation MAE: 11.14 (100*100 20epoch 20 unfreezed 0.3 dropout 0.001 Regulaization 0.0001 LR)"],"metadata":{"id":"Hg328--UqFcF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mean_absolute_percentage_error(y_true, y_pred):\n","    \"\"\"Calculates MAPE given y_true and y_pred.\"\"\"\n","    # Ensure both arrays have the same length before calculation\n","    min_len = min(len(y_true), len(y_pred))\n","    y_true = y_true[:min_len]\n","    y_pred = y_pred[:min_len]\n","\n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","# Get predictions for the entire test dataset without batching\n","y_true = test_ds[[\"pH\", \"TDS\"]].values\n","y_pred = model.predict(test_generator, steps=len(test_generator), verbose=0)\n","\n","# Calculate and print MAPE for each output (pH and TDS)\n","mape_pH = mean_absolute_percentage_error(y_true[:, 0], y_pred[:, 0])\n","mape_TDS = mean_absolute_percentage_error(y_true[:, 1], y_pred[:, 1])\n","min_samples = min(y_true.shape[0], y_pred.shape[0])\n","y_true = y_true[:min_samples]\n","y_pred = y_pred[:min_samples]\n","\n","print(f\"MAPE for pH: {mape_pH:.2f}%\")\n","print(f\"MAPE for TDS: {mape_TDS:.2f}%\")\n","\n"],"metadata":{"id":"TbwxLgRbQrrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import r2_score\n","\n","# Get the true values for the test set\n","y_true = test_ds[[\"pH\", \"TDS\"]].values  # Extract pH and TDS columns as NumPy array\n","\n","y_pred = model.predict(test_generator, steps=len(test_generator), verbose=0)\n","\n","# Ensure y_true and y_pred have the same number of samples\n","min_samples = min(y_true.shape[0], y_pred.shape[0])\n","y_true = y_true[:min_samples]\n","y_pred = y_pred[:min_samples]\n","\n","# Calculate R-squared for each output (pH and TDS)\n","r2_pH = r2_score(y_true[:, 0], y_pred[:, 0])\n","r2_TDS = r2_score(y_true[:, 1], y_pred[:, 1])\n","\n","print(f\"R-squared for pH: {r2_pH:.2f}\")\n","print(f\"R-squared for TDS: {r2_TDS:.2f}\")\n"],"metadata":{"id":"T6xYGEOnSf7m"},"execution_count":null,"outputs":[]}]}